# ğŸ•¸ï¸ Web Scraping + Data Lake Project

A robust and scalable data engineering pipeline for extracting, processing, and storing web data in a cloud-based data lake. This project simulates an end-to-end pipeline for collecting product information from e-commerce websites and storing it in a well-organized and queryable data lake on AWS S3. The project includes scraping, data cleansing, metadata management, partitioning, and orchestration.

---

## ğŸ” Objective

Build a production-grade data pipeline that scrapes data from target web sources, transforms it into a structured format, and stores it in a scalable cloud data lake architecture. The goal is to create a foundation for analytics and business intelligence workloads.

---

## âš™ï¸ Tech Stack

| Layer              | Tool/Service                      |
|-------------------|------------------------------------|
| Web Scraping       | Python, BeautifulSoup, Requests, Selenium (optional) |
| Data Transformation| Pandas, PyArrow                   |
| Cloud Storage      | AWS S3 (Data Lake)                |
| Orchestration      | Apache Airflow                    |
| Metadata Catalog   | AWS Glue Catalog                  |
| Query Layer        | AWS Athena                        |
| Monitoring         | CloudWatch, Logging (JSON format) |

---

## ğŸ§± Project Architecture

```plaintext
          +---------------------+
          |  Web Scraper (Python)|
          +----------+----------+
                     |
                     v
          +----------+----------+
          | Transformation Layer|
          | (clean, dedup, enrich) |
          +----------+----------+
                     |
                     v
          +----------+----------+
          | Data Lake (S3)      |
          | Partitioned by date |
          +----------+----------+
                     |
                     v
          +----------+----------+
          | AWS Glue Catalog    |
          +----------+----------+
                     |
                     v
          +----------+----------+
          | Query Layer (Athena)|
          +---------------------+
```

---

## ğŸ“¦ Folder Structure

```
web-scraping-datalake/
â”‚
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ etl_scrape_pipeline.py
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ scraper.py             # Scraping logic
â”‚   â””â”€â”€ utils.py               # Helper functions
â”œâ”€â”€ transformer/
â”‚   â””â”€â”€ transform.py           # Data cleaning, formatting
â”œâ”€â”€ data/                      # Local data storage (for testing)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ settings.yaml          # Configurations for sites, headers, proxies
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl_logs.json          # Structured logging
â”œâ”€â”€ s3_bucket_structure.md     # Description of S3 layout
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Key Features

- âœ… Modular scraping pipeline for multiple websites  
- âœ… Daily batch ingestion using Apache Airflow  
- âœ… Schema consistency and version control  
- âœ… Partitioned Parquet storage in S3 (e.g., `s3://datalake/ecommerce/products/dt=2025-05-06/`)  
- âœ… Queryable via AWS Athena with Glue Table integration  
- âœ… Fault-tolerant with retry logic and error alerting  
- âœ… Monitoring and structured logging  

---

## ğŸš€ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/param-2003/web-scraping-datalake.git
cd web-scraping-datalake
```

### 2. Install Dependencies
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Configure AWS
Ensure your AWS CLI is configured with access to S3 and Glue.
```bash
aws configure
```

### 4. Create S3 Buckets
Update your `settings.yaml` with your S3 bucket name and prefix.

### 5. Run Scraper Locally
```bash
python scraper/scraper.py --site "example_site"
```

### 6. Trigger via Airflow
Place the DAG in your Airflow `dags/` directory and trigger it manually or via scheduler.

---

## ğŸ§¹ Sample Output Format (Parquet Schema)

```parquet
product_id: string
title: string
price: float
currency: string
availability: string
scraped_date: date
source_url: string
```

---

## ğŸ“Š Querying with Athena

You can query structured data using Athena:
```sql
SELECT title, price, availability
FROM ecommerce_products
WHERE dt = '2025-05-06';
```

---

## ğŸ“ˆ Future Improvements

- Add support for real-time scraping with Kafka  
- Integrate data quality checks using Great Expectations  
- Add Change Data Capture (CDC) features  
- Visual dashboard using Looker Studio or Tableau  
- Include proxy rotation and CAPTCHA solving support  

---


## ğŸ“„ License

MIT License â€“ see the LICENSE file for details.
